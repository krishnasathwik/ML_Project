{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf5b0ac-1bc0-4e7e-b4fa-73e5426d2a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Test data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mannepalli/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
      "Best cross-validation accuracy: 0.8839194473127057\n",
      "Validation Accuracy: 0.88\n",
      "Submission file created: sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "class OutlierRemoval:\n",
    "    def __init__(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Drop the LoanID column\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Reduce the hyperparameter grid size\n",
    "param_dist = {\n",
    "    'C': [0.1, 1],  # Fewer values for C\n",
    "    'kernel': ['linear', 'rbf'],  # Fewer kernels\n",
    "    'gamma': ['scale'],  # Use only 'scale' for simplicity\n",
    "}\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=svm, param_distributions=param_dist, n_iter=5, cv=2, \n",
    "                                   scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", random_search.best_score_)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_data)\n",
    "\n",
    "# Prepare the sample submission file\n",
    "sample_submission = pd.DataFrame({'LoanID': test_data.index, 'Default': test_predictions})\n",
    "sample_submission.to_csv('sample_submission_svm.csv', index=False)\n",
    "print(\"Submission file created: sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f072ace-24cd-49d3-8b10-56a0bbf4ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Test data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "dtype: int64\n",
      "Best parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best cross-validation accuracy: 0.8839194473458035\n",
      "Validation Accuracy: 0.88\n",
      "Submission file created: sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "class OutlierRemoval:\n",
    "    def __init__(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Preserve the LoanID column from the original test data\n",
    "test_loan_ids = test_data['LoanID']\n",
    "\n",
    "# Drop the LoanID column\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Define reduced hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_data)\n",
    "\n",
    "# Prepare the sample submission file\n",
    "sample_submission = pd.DataFrame({'LoanID': test_loan_ids, 'Default': test_predictions})\n",
    "sample_submission.to_csv('sample_submission.csv', index=False)\n",
    "print(\"Submission file created: sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac027d5a-0f17-4ea9-b2cb-2a321e6d151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Test data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "class OutlierRemoval:\n",
    "    def __init__(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Preserve the LoanID column from the original test data\n",
    "test_loan_ids = test_data['LoanID']\n",
    "\n",
    "# Drop the LoanID column\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define the SVM model with class weights\n",
    "svm = SVC(class_weight='balanced')\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_data)\n",
    "\n",
    "# Prepare the sample submission file\n",
    "sample_submission = pd.DataFrame({'LoanID': test_loan_ids, 'Default': test_predictions})\n",
    "sample_submission.to_csv('sample_submission.csv', index=False)\n",
    "print(\"Submission file created: sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318929e-0b0d-45e0-a3a5-19c1d69593f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee7f09d-9663-41bd-9cd2-31d974cb045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "Default           0\n",
      "dtype: int64\n",
      "Test data null values:\n",
      " LoanID            0\n",
      "Age               0\n",
      "Income            0\n",
      "LoanAmount        0\n",
      "CreditScore       0\n",
      "MonthsEmployed    0\n",
      "NumCreditLines    0\n",
      "InterestRate      0\n",
      "LoanTerm          0\n",
      "DTIRatio          0\n",
      "Education         0\n",
      "EmploymentType    0\n",
      "MaritalStatus     0\n",
      "HasMortgage       0\n",
      "HasDependents     0\n",
      "LoanPurpose       0\n",
      "HasCoSigner       0\n",
      "dtype: int64\n",
      "Best parameters: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation accuracy: 0.692193781269982\n",
      "Validation Accuracy: 0.69\n",
      "Submission file created: sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "class OutlierRemoval:\n",
    "    def __init__(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Preserve the LoanID column from the original test data\n",
    "test_loan_ids = test_data['LoanID']\n",
    "\n",
    "# Drop the LoanID column\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define the SVM model with class weights\n",
    "svm = SVC(class_weight='balanced')\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation data\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test_data)\n",
    "\n",
    "# Prepare the sample submission file\n",
    "sample_submission = pd.DataFrame({'LoanID': test_loan_ids, 'Default': test_predictions})\n",
    "sample_submission.to_csv('sample_submission.csv', index=False)\n",
    "print(\"Submission file created: sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f4e8a-04b1-42de-b649-67bb756d0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "# Store LoanID from test data for the final submission\n",
    "test_loan_ids = test_data['LoanID']\n",
    "\n",
    "# Define the OutlierRemoval class\n",
    "class OutlierRemoval:\n",
    "    def _init_(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, \"Bachelor's\": 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Feature engineering: Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Drop the LoanID column from train data (not needed for training)\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "param_dist_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': uniform(0.0001, 0.01),\n",
    "    'learning_rate_init': uniform(0.001, 0.1),\n",
    "    'max_iter': randint(200, 2000)\n",
    "}\n",
    "\n",
    "# Train MLPClassifier with RandomizedSearchCV\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "random_search_mlp = RandomizedSearchCV(\n",
    "    estimator=mlp, param_distributions=param_dist_mlp, n_iter=50, cv=3, n_jobs=-1, scoring='accuracy', random_state=42\n",
    ")\n",
    "random_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters for MLPClassifier\n",
    "print(\"Best hyperparameters for MLPClassifier: \", random_search_mlp.best_params_)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_mlp = random_search_mlp.best_estimator_.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_mlp = accuracy_score(y_val, y_pred_mlp)\n",
    "print(f\"Validation accuracy for MLPClassifier: {accuracy_mlp:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions_mlp = random_search_mlp.best_estimator_.predict(test_data)\n",
    "\n",
    "# Prepare submission file\n",
    "sample_submission_mlp = pd.DataFrame({'LoanID': test_loan_ids, 'Default': test_predictions_mlp})\n",
    "sample_submission_mlp.to_csv('sample_submission_mlp_final.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'sample_submission_mlp_final.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
