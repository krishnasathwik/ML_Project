{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306c41b6-d7b5-4d33-8e49-83eb20ae70f7",
   "metadata": {},
   "source": [
    "This code is using HistGradientBoostingClassifier and RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0304912-c614-4820-b5c2-a28f810faf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the train and test datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for null values\n",
    "print(\"Train data null values:\\n\", train.isnull().sum())\n",
    "print(\"Test data null values:\\n\", test_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates from train data if any\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "# Store LoanID from test data for the final submission\n",
    "test_loan_ids = test_data['LoanID']\n",
    "\n",
    "# Define the OutlierRemoval class\n",
    "class OutlierRemoval:\n",
    "    def __init__(self, col):\n",
    "        q1 = col.quantile(0.25)\n",
    "        q3 = col.quantile(0.75)\n",
    "        inter_quartile_range = q3 - q1\n",
    "        self.upper_whisker = q3 + inter_quartile_range * 1.5\n",
    "        self.lower_whisker = q1 - inter_quartile_range * 1.5\n",
    "\n",
    "    def remove(self, row):\n",
    "        if self.lower_whisker <= row <= self.upper_whisker:\n",
    "            return row\n",
    "        elif row < self.lower_whisker:\n",
    "            return self.lower_whisker\n",
    "        else:\n",
    "            return self.upper_whisker\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \n",
    "                     'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# Apply outlier removal on numerical columns in train and test data\n",
    "for column in numerical_columns:\n",
    "    remover = OutlierRemoval(train[column])\n",
    "    train[column] = train[column].apply(remover.remove)\n",
    "    \n",
    "    test_remover = OutlierRemoval(test_data[column])\n",
    "    test_data[column] = test_data[column].apply(test_remover.remove)\n",
    "\n",
    "# Define mappings for categorical columns\n",
    "mappings = {\n",
    "    'MaritalStatus': {'Married': 0, 'Single': 1, 'Divorced': 2},\n",
    "    'Education': {'High School': 0, \"Bachelor's\": 1, 'Master\\'s': 2},\n",
    "    'EmploymentType': {'Full-time': 0, 'Part-time': 1, 'Self-employed': 2, 'Unemployed': 3},\n",
    "    'HasMortgage': {'Yes': 1, 'No': 0},\n",
    "    'HasDependents': {'Yes': 1, 'No': 0},\n",
    "    'LoanPurpose': {'Home': 0, 'Auto': 1, 'Education': 2, 'Business': 3, 'Other': 4},\n",
    "    'HasCoSigner': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "\n",
    "# Apply mappings to train and test data\n",
    "for col, mapping in mappings.items():\n",
    "    train[col] = train[col].map(mapping)\n",
    "    test_data[col] = test_data[col].map(mapping)\n",
    "\n",
    "# Feature engineering: Combine MaritalStatus and HasDependents into a new feature\n",
    "train['Marital_Dependents'] = train['MaritalStatus'] + train['HasDependents']\n",
    "test_data['Marital_Dependents'] = test_data['MaritalStatus'] + test_data['HasDependents']\n",
    "\n",
    "# Drop the LoanID column from train data (not needed for training)\n",
    "train = train.drop(columns=['LoanID'])\n",
    "test_data = test_data.drop(columns=['LoanID'])\n",
    "\n",
    "# Separate target variable\n",
    "X = train.drop(columns=['Default'])\n",
    "y = train['Default']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_val[numerical_columns] = scaler.transform(X_val[numerical_columns])\n",
    "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(knn_imputer.transform(X_val), columns=X_val.columns)\n",
    "test_data = pd.DataFrame(knn_imputer.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "param_dist_hgb = {\n",
    "    'learning_rate': uniform(0.01, 0.2), \n",
    "    'max_depth': randint(3, 10),          \n",
    "    'max_iter': randint(100, 300),        \n",
    "    'min_samples_leaf': randint(20, 100), \n",
    "    'l2_regularization': uniform(0.0, 5.0),\n",
    "}\n",
    "\n",
    "# Train HistGradientBoostingClassifier with RandomizedSearchCV\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "random_search_hgb = RandomizedSearchCV(\n",
    "    estimator=hgb, param_distributions=param_dist_hgb, n_iter=30, cv=3, n_jobs=-1, scoring='accuracy', random_state=42\n",
    ")\n",
    "random_search_hgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters for HistGradientBoostingClassifier\n",
    "print(\"Best hyperparameters for HistGradientBoostingClassifier: \", random_search_hgb.best_params_)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_hgb = random_search_hgb.best_estimator_.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_hgb = accuracy_score(y_val, y_pred_hgb)\n",
    "print(f\"Validation accuracy for HistGradientBoostingClassifier: {accuracy_hgb:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions_hgb = random_search_hgb.best_estimator_.predict(test_data)\n",
    "\n",
    "# Prepare submission file\n",
    "sample_submission_hgb = pd.DataFrame({'LoanID': test_loan_ids, 'Default': test_predictions_hgb})\n",
    "sample_submission_hgb.to_csv('sample_submission_hgb.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'sample_submission_hgb.csv' created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
